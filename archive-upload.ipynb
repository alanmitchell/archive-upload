{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development of Archive-Upload Script\n",
    "\n",
    "This notebook is used for initial development of script features\n",
    "and for various experiments.  The final script can be found in\n",
    "`archive-upload.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path, PurePosixPath\n",
    "import yaml\n",
    "import pickle\n",
    "import bz2\n",
    "from urllib.parse import urlparse\n",
    "from pprint import pprint\n",
    "import logging, logging.handlers\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on Design\n",
    "\n",
    "* When a finished file is found, compressed, and archived, immediately try\n",
    "  to upload.  If the upload fails, add the file to the \"database\" of files\n",
    "  that need to be uploaded.  That database of pending uploads should hold\n",
    "  the local file path and the destination S3 bucket.  It could just be a list\n",
    "  of two-tuples that is pickled to disk in the ~/.archive-upload directory.\n",
    "* For the newly finished file, may just want to append it to the \"upload pending\"\n",
    "  list, and then after that start trying to work through the list, uploading\n",
    "  the oldest file first.\n",
    "* The AWS sync command may not be the best because if someone cleans out the S3\n",
    "  bucket, the script will try to re-upload all of the local files in the archive\n",
    "  directory.  The approach above solves that problem by only uploading a file once\n",
    "  to S3.  It also has the advantage of only requiring Python boto3 methods (not\n",
    "  needing a AWS command line utility)\n",
    "* Archiving and Uploading application log files may be tricky.  Rotating file handler will\n",
    "  keep producing files of the same names: archive-upload.log, archive-upload.log.1, etc.  One idea\n",
    "  might be to take the last X lines of log file at the end of the day (or all lines) and copy\n",
    "  those lines into a day-specific log file: 2019-07-23_archive-upload.log. This could\n",
    "  be done with a separate Cron job.  Then this archive-upload utility\n",
    "  could archive and upload those files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'directories': [{'archive-dir': '/home/tabb99/arch-test/archive/data',\n",
      "                  'bucket-and-key': 'dataacq.analysisnorth.com/powerhouse/kwethluk/data',\n",
      "                  'delete-after': 365,\n",
      "                  'directory': '/home/tabb99/arch-test/data',\n",
      "                  'file-patterns': [{'finished-secs': 60, 'pattern': '*.csv'},\n",
      "                                    {'pattern': '*.txt'}]},\n",
      "                 {'archive-dir': '/home/tabb99/arch-test/archive/daily-logs',\n",
      "                  'bucket-and-key': 'dataacq.analysisnorth.com/powerhouse/kwethluk/daily-logs',\n",
      "                  'delete-after': 30,\n",
      "                  'directory': '/home/tabb99/arch-test/daily-logs',\n",
      "                  'file-patterns': [{'finished-secs': 10,\n",
      "                                     'pattern': '*.log'}]}],\n",
      " 'log-file-dir': '/home/tabb99/arch-test/log',\n",
      " 'log-level': 'INFO'}\n"
     ]
    }
   ],
   "source": [
    "# Read in the configuration file that controls execution of the script.\n",
    "cfg_fn = 'archive-config-example.yaml'\n",
    "config = yaml.safe_load(open(cfg_fn, 'r'))\n",
    "pprint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Files to Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /home/tabb99/arch-test\n",
    "touch data/2019-07-06_data.csv\n",
    "touch data/2019-07-07_data.csv\n",
    "touch daily-logs/2019-07-07_errors.log\n",
    "touch daily-logs/2019-07-06_errors.log\n",
    "touch data/junk\n",
    "touch daily-logs/junk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-07 15:39:31,082 - INFO - <ipython-input-4-1769626a71b6> - Script start.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the log level. Because we are setting this on the logger, it will apply\n",
    "# to all handlers (unless maybe you set a specific level on a handler?).\n",
    "logging.root.setLevel(getattr(logging, config['log-level']))\n",
    "\n",
    "# create a rotating file handler\n",
    "# Create Log file directory if it does not exist\n",
    "p_log_dir = Path(config['log-file-dir'])\n",
    "p_log_dir.mkdir(parents=True, exist_ok=True)\n",
    "p_log = p_log_dir / 'archive-upload.log'\n",
    "\n",
    "fh = logging.handlers.RotatingFileHandler(p_log, maxBytes=200000, backupCount=5)\n",
    "\n",
    "# create formatter and add it to the handler\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(module)s - %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "\n",
    "# create a handler that will print to console as well.\n",
    "console_h = logging.StreamHandler()\n",
    "console_h.setFormatter(formatter)\n",
    "\n",
    "# add the handlers to the root logger\n",
    "logging.root.addHandler(fh)\n",
    "logging.root.addHandler(console_h)\n",
    "\n",
    "logging.info('Script start.')\n",
    "\n",
    "# Create Application working directory if it does not exist.\n",
    "p_app = Path('~').expanduser() / '.archive-upload'\n",
    "p_app.mkdir(exist_ok=True)\n",
    "\n",
    "# Path to pickle file holding the list of files that need to be uploaded\n",
    "# but haven't been yet.\n",
    "p_up_pending = p_app / 'upload_pending.pkl'\n",
    "\n",
    "# Read a list of pending uploads, if file is present, otherwise,\n",
    "# set to empty list.\n",
    "if p_up_pending.exists():\n",
    "    with p_up_pending.open('rb') as fin:\n",
    "        upload_pending = pickle.load(fin)\n",
    "else:\n",
    "    upload_pending = []\n",
    "upload_pending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_pending = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-07 11:49:51,780 - INFO - <ipython-input-57-30f3160d6c3c> - /home/tabb99/arch-test/archive/xyz/another01.txt.bz2 deleted due to exceeding max age.\n",
      "2019-07-07 11:49:51,782 - INFO - <ipython-input-57-30f3160d6c3c> - /home/tabb99/arch-test/archive/xyz/another02.txt.bz2 deleted due to exceeding max age.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Loop through the list of directories, looking for completed files.\n",
    "for dr in config['directories']:\n",
    "    # Path to directory holding data files\n",
    "    p_dr = Path(dr['directory'])\n",
    "    \n",
    "    # Path to directory where finished, compressed files will be archived\n",
    "    p_archive = Path(dr['archive-dir'])\n",
    "    \n",
    "    # make the archive directory if it does not exists\n",
    "    p_archive.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Loop through file patterns\n",
    "    for pat in dr['file-patterns']:\n",
    "        for p_f in p_dr.glob(pat['pattern']):\n",
    "            # p_f is a Path to a file matching the pattern.\n",
    "            # test to see if it is a completed file\n",
    "            file_age = time.time() - p_f.stat().st_mtime\n",
    "            if file_age > pat['finished-secs']:\n",
    "                p_arch_fn = p_archive / (p_f.name + '.bz2')\n",
    "                try:\n",
    "                    with bz2.open(p_arch_fn, \"wb\") as fout:\n",
    "                      fout.write(p_f.read_bytes())\n",
    "\n",
    "                    # add the archive file to the upload list. I'm converting the Path objects\n",
    "                    # to strings so the pickle is more straight-forward.\n",
    "                    new_upload = (str(p_arch_fn), str(PurePosixPath(dr['bucket-and-key']) / p_arch_fn.name))\n",
    "                    upload_pending.append(new_upload)\n",
    "                                  \n",
    "                    # delete the source file\n",
    "                    p_f.unlink()\n",
    "                    \n",
    "                    logging.info(f'Archived {p_f}')\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logging.exception(f'Error attempting to archive {p_f}')            \n",
    "    \n",
    "    # Check for files to delete in the archive directory\n",
    "    # but only delete if the file is not in the upload pending list\n",
    "    if len(upload_pending):\n",
    "        pending_file_list = list(zip(*upload_pending))[0]\n",
    "    else:\n",
    "        pending_file_list = []\n",
    "    if 'delete-after' in dr and dr['delete-after'] > 0:\n",
    "        max_age = dr['delete-after'] * 24 * 3600.\n",
    "        for p_f in p_archive.glob('*.bz2'):\n",
    "            file_age = time.time() - p_f.stat().st_mtime\n",
    "            if not str(p_f) in pending_file_list and file_age > max_age:\n",
    "                p_f.unlink()\n",
    "                logging.info(f'{p_f} deleted due to exceeding max age.')\n",
    "\n",
    "pprint(upload_pending)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do\n",
    "\n",
    "* In code, supply default values for many of the configuration entries,\n",
    "  such as 'finished-secs' (5 seconds).\n",
    "* copy log file once daily to a location that is watched by this\n",
    "  utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Upload files\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# need to copy the list to iterate across it because this\n",
    "# codes deletes items out of the original list.\n",
    "for fn, bucket_key in upload_pending.copy():\n",
    "    # if this file no longer exists, delete it from the upload list.\n",
    "    if not Path(fn).exists():\n",
    "        upload_pending.remove((fn, bucket_key))\n",
    "        logging.info(f'{fn} does not exist, so will not be uploaded.')\n",
    "        \n",
    "    # split the bucket + key into a bucket and a key.  The urlparse\n",
    "    # function does this well, except for leaving a leading slash on the\n",
    "    # key.\n",
    "    parts = urlparse('s3://' + bucket_key)\n",
    "    bucket = parts.netloc\n",
    "    key = parts.path[1:]   # remove leading slash\n",
    "    try:\n",
    "        s3.meta.client.upload_file(fn, bucket, key)\n",
    "        upload_pending.remove((fn, bucket_key))\n",
    "        logging.info(f'Uploaded {fn}')\n",
    "    except Exception as e:\n",
    "        logging.exception('Error attempting to upload {fn}')\n",
    "print(upload_pending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the upload pending list\n",
    "# Really should put this in a Finally clause so that with any weird errors\n",
    "# this list will be saved.\n",
    "with p_up_pending.open('wb') as fout:\n",
    "    pickle.dump(upload_pending, fout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 5)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = [ (1, 2), (3, 4), (5, 6)]\n",
    "list(zip(*l1))[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
